import os
import json
import numpy as np
import torch
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.preprocessing import StandardScaler
from scipy.stats import gaussian_kde

def load_dicts(base_path, filename="first_u_vectors.pt", start_step=None, end_step=None):
    """åŠ è½½æ‰€æœ‰ step çš„å‘é‡å­—å…¸ï¼ˆæŒ‰æ•°å€¼æ’åºï¼Œå¯é€‰ start_step å’Œ end_stepï¼‰"""
    all_dicts = []
    step_names = os.listdir(base_path)

    # åªä¿ç•™ä»¥ global_step_ å¼€å¤´çš„æ–‡ä»¶å¤¹
    step_names = [s for s in step_names if s.startswith("global_step_")]

    # æŒ‰æ•°å€¼æ’åº
    step_names = sorted(step_names, key=lambda x: int(x.split("_")[-1]))

    for step_name in step_names:
        try:
            step = int(step_name.split("_")[-1])
        except ValueError:
            print(f"âš ï¸ è·³è¿‡ {step_name}ï¼Œæ— æ³•è§£æ step")
            continue

        # å¦‚æœè®¾ç½®äº† start_step / end_stepï¼Œå°±è¿‡æ»¤
        if start_step is not None and step < start_step:
            continue
        if end_step is not None and step > end_step:
            continue

        step_path = os.path.join(base_path, step_name, filename)
        if not os.path.exists(step_path):
            raise FileNotFoundError(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {step_path}")

        data = torch.load(step_path, map_location="cpu")
        all_dicts.append((step, data))

    if not all_dicts:
        raise RuntimeError(f"æœªæ‰¾åˆ°ä»»ä½• step âˆˆ [{start_step}, {end_step}] çš„ {filename} æ–‡ä»¶")

    return all_dicts


def get_common_keys(all_dicts):
    """æ‰¾åˆ°æ‰€æœ‰ step ä¸­å…±åŒçš„ key"""
    common_keys = None
    for _, d in all_dicts:
        keys = set(d.keys())
        common_keys = keys if common_keys is None else common_keys & keys
    return common_keys if common_keys else set()


def gather_by_steps_for_key(all_dicts, y, key, selected_steps):
    """æ”¶é›†æŸä¸ª key åœ¨ä¸åŒ step çš„å‘é‡å’Œå¯¹åº”å‡†ç¡®ç‡"""
    X, y_sub, used_steps = [], [], []
    for step, d in all_dicts:
        if step in selected_steps and key in d:
            v = d[key].cpu().numpy().astype(np.float64).ravel()
            if np.linalg.norm(v) < 1e-12:
                continue
            X.append(v)
            y_sub.append(y[step])
            used_steps.append(step)
    if not X:
        return np.array([]), np.array([]), []
    # import pdb
    # pdb.set_trace()
    return np.vstack(X), np.array(y_sub), used_steps


def predict_all_keys_pls(
    base_path,
    y,
    target_y=1.0,
    filename="first_u_vectors.pt",
    start_step=1,
    end_step=8,
    step_indices=None,
    save_file="predicted_vectors_y1.pt",
    min_samples=3,
    n_components=2,        # å…è®¸å¤šåˆ†é‡ï¼Œä¸ä½ åŸå…ˆå¯è§†åŒ–ä¸€è‡´
    scale=False,           # æˆ‘ä»¬æ‰‹åŠ¨åªæ ‡å‡†åŒ– Xï¼Œy ä¸ç¼©æ”¾
    r2_filter_file=None,
    r2_threshold=None,
    plot_file="r2_distribution.png",
    plot_kde=True,
    eps=1e-12
):
    # ---------- åŠ è½½ ----------
    all_dicts_full = load_dicts(base_path, filename=filename,start_step=start_step,end_step=end_step)
    if not all_dicts_full:
        raise RuntimeError(f"æœªæ‰¾åˆ°ä»»ä½• {filename}")

    steps_all = [s for s, _ in all_dicts_full]
    if step_indices:
        selected_steps = sorted(set(int(s) for s in step_indices))
    else:
        selected_steps = [s for s in steps_all if start_step <= s <= end_step]

    selected_steps_set = set(selected_steps)
    print(f"ğŸ§® ä½¿ç”¨çš„ step: {selected_steps}")

    all_dicts = [(s, d) for (s, d) in all_dicts_full if s in selected_steps_set]
    common_keys = get_common_keys(all_dicts)
    if not common_keys:
        raise RuntimeError("æœªæ‰¾åˆ°å…¬å…± key")

    # å¯é€‰ï¼šæŒ‰å¤–éƒ¨ RÂ² æ–‡ä»¶è¿‡æ»¤ï¼ˆå£å¾„ç”±ä½ æä¾›çš„æ–‡ä»¶å†³å®šï¼‰
    allowed = set(common_keys)
    if r2_filter_file and os.path.exists(r2_filter_file) and r2_threshold is not None:
        with open(r2_filter_file, "r") as f:
            r2_map = json.load(f)
        allowed = {k for k in common_keys if r2_map.get(k, 0.0) >= float(r2_threshold)}
        print(f"R^2 è¿‡æ»¤ï¼šé˜ˆå€¼ {r2_threshold}ï¼Œä¿ç•™ {len(allowed)}/{len(common_keys)} ä¸ª keyã€‚")

    predictions = {}
    r2_scores = {}  # ä¿å­˜ comp1 çš„ RÂ²ï¼ˆä¸åŸä»£ç ä¸€è‡´ï¼‰
    ok, fail = 0, 0

    for key in common_keys:
        if key not in allowed:
            continue

        X, y_sub, used = gather_by_steps_for_key(all_dicts, y, key, selected_steps)

        if X.ndim != 2 or len(y_sub) < min_samples:
            fail += 1
            continue
        if np.std(y_sub) < eps or np.linalg.norm(X) < eps:
            print(f"âš ï¸ Skip {key}: é›¶æ–¹å·®/é›¶èŒƒæ•°")
            fail += 1
            continue

        try:
            # ---------- æ ‡å‡†åŒ– Xï¼ˆä¸åŸä»£ç ä¸€è‡´ï¼‰ ----------
            scaler_X = StandardScaler()
            X_scaled = scaler_X.fit_transform(X)  # (N, D)

            # ---------- PLSï¼šX -> y ----------
            # æœ‰æ—¶ç‰¹å¾/æ ·æœ¬æ•°ä¼šé™åˆ¶ n_componentsï¼Œä¸Šé™åšä¸ªå®‰å…¨è£å‰ª
            n_comp_eff = int(min(n_components, X_scaled.shape[0], X_scaled.shape[1]))
            n_comp_eff = max(1, n_comp_eff)

            pls = PLSRegression(n_components=n_comp_eff, scale=False)
            # sklearn æ¥å— (N,) æˆ– (N,1)ï¼Œè¿™é‡Œç”¨ (N,1)
            pls.fit(X_scaled, y_sub.reshape(-1, 1))

            # å¾—åˆ†çŸ©é˜µï¼ˆTï¼‰ï¼Œè½½è·ï¼ˆPï¼‰
            T = pls.x_scores_                  # (N, n_comp_eff)
            P = pls.x_loadings_                # (D, n_comp_eff)

            # ---------- RÂ² è®¡ç®—ï¼ˆä¸åŸå£å¾„ä¸€è‡´ï¼šy ~ T[:,0]ï¼‰ ----------
            # comp1 å•å˜é‡å›å½’
            from sklearn.linear_model import LinearRegression
            from sklearn.metrics import r2_score

            reg1 = LinearRegression().fit(T[:, [0]], y_sub)
            y_hat_1 = reg1.predict(T[:, [0]])
            r2_1 = r2_score(y_sub, y_hat_1)
            r2_scores[key] = float(r2_1)

            # ä»…ä¾›å‚è€ƒï¼šå¤šåˆ†é‡æ•´ä½“å›å½’çš„ RÂ²ï¼ˆæ‰“å°ï¼Œä¸å­˜ï¼‰
            reg_all = LinearRegression().fit(T, y_sub)
            y_hat_all = reg_all.predict(T)
            r2_all = r2_score(y_sub, y_hat_all)

            # ---------- é¢„æµ‹ y=target_y çš„å‘é‡ ----------
            a = float(reg_all.intercept_)
            beta = reg_all.coef_.astype(np.float64)  # (n_comp_eff,)

            if n_comp_eff == 1:
                # æ ‡é‡åè§£
                if abs(beta[0]) < eps:
                    raise RuntimeError("betaâ‰ˆ0ï¼Œæ— æ³•åè§£ t*")
                t_star = np.array([(target_y - a) / beta[0]], dtype=np.float64)  # (1,)
            else:
                # æœ€å°äºŒä¹˜æŠ•å½±åˆ° beta æ–¹å‘
                beta_norm2 = float(np.dot(beta, beta))
                if beta_norm2 < eps:
                    raise RuntimeError("â€–betaâ€–â‰ˆ0ï¼Œæ— æ³•åè§£ t*")
                t_star = ((target_y - a) / beta_norm2) * beta  # (n_comp_eff,)

            # å›åˆ° X çš„æ ‡å‡†åŒ–ç©ºé—´ï¼šx_scaled* = t* P^T
            X_scaled_pred = np.dot(t_star.reshape(1, -1), P.T)  # (1, D)
            X_pred = scaler_X.inverse_transform(X_scaled_pred)[0]  # (D,)

            predictions[key] = torch.tensor(X_pred, dtype=torch.float32)

            # ---------- è¯Šæ–­è¾“å‡º ----------
            idx_best = int(np.argmax(y_sub))
            x_best = X[idx_best]
            denom = np.linalg.norm(X_pred) * np.linalg.norm(x_best)
            cos_sim = float(np.dot(X_pred, x_best) / denom) if denom > eps else 0.0

            print(
                f"[{key}] R^2(comp1)={r2_1:.4f} | R^2(all)={r2_all:.4f} | "
                f"Predâ€–xâ€–={np.linalg.norm(X_pred):.6f} | "
                f"CosSim(pred, best)={cos_sim:.6f} | steps={used}"
            )
            ok += 1
        except Exception as e:
            print(f"âš ï¸ Skip {key} (steps={used}): {e}")
            fail += 1

    # ---------- ä¿å­˜ ----------
    os.makedirs(os.path.dirname(save_file), exist_ok=True)
    torch.save(predictions, save_file)
    print(f"âœ… Saved {len(predictions)} predicted vectors to: {save_file} (ok={ok}, fail={fail})")

    # import pdb
    # pdb.set_trace()
    return predictions, r2_scores



# ----------------- ä½¿ç”¨ç¤ºä¾‹ -----------------
if __name__ == "__main__":
    base_path = ""
    y = np.array([], dtype=np.float64)

    for i in range(10,12):
        predict_all_keys_pls(
            base_path=base_path,
            y=y,
            target_y=0.8,
            filename="first_au_vectors.pt",
            start_step=5,
            end_step=i,
            save_file=f"",
            min_samples=3,
            n_components=1,
            scale=False,
            plot_file="r2_distribution.png"
        )
